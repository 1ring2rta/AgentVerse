cnt_agents: &cnt_agents 2
max_turn: &max_turn 3
max_criticizing_rounds: 3

prompts:
  role_assigner_prepend_prompt: &role_assigner_prepend_prompt |-
    # Role Description
    You are the leader of a group of experts, now you are facing a grade school math problem:
    ${task_description}
    
    You can recruit ${cnt_critic_agents} expert in different fields. What experts will you recruit to better generate an accurate solution?

    Here are some suggestion:
    ${advice}
    
  role_assigner_append_prompt: &role_assigner_append_prompt |-
    # Response Format Guidance
    You should respond with a list of expert description. For example:
    1. an electrical engineer specified in the filed of xxx.
    2. an economist who is good at xxx.
    3. a lawyer with a good knowledge of xxx.
    ...

    Only respond with the description of each role. Do not include your reason.

  solver_prepend_prompt: &solver_prepend_prompt |-
    Can you solve the following math problem? 
    ${task_description} 

  solver_append_prompt: &solver_append_prompt |-
    You are ${role_description}. Using these information, can you provide the correct solution to the math problem? You should always respond in the following format:
    Thought: your thought on the problem
    Reasoning: your reasoning process
    Criticism: constructive self-criticism on the above content
    Solution: your solution and the process. your final answer must be a single numerical number (not a equation, fraction, function or variable), in the form \boxed{answer}, at the end of your response.
  # Explain your reasoning. Your final answer must be a single numerical number (not a equation, fraction, function or variable), in the form \boxed{answer}, at the end of your response.

  # Your response should include one final answer in the form of \boxed{answer} at the end. If there are multiple answers given by different agents, you should only select the correct one and summarize its deduction process. Do not summarize each agent's response separately, summarize the whole chat history and give one final answer.
  # Can you solve the following math problem? 
  # ${task_description} 

  # # Previous Solution
  # The solution you gave in the last step is:
  # ```
  # ${former_solution}
  # ```

  # # Critics
  # There are some critics on the above solution:
  # ```
  # ${critic_opinions}
  # ```
  
  # Using the these information, can you provide the correct solution to the math problem? Explain your reasoning. Your final answer should be a single numerical number (not a equation), in the form \boxed{answer}, at the end of your response.

  critic_prepend_prompt: &critic_prepend_prompt |-
    You are in a discussion group, aiming to collaborative solve the following math problem:
    ${task_description}

  critic_append_prompt: &critic_append_prompt |-
    You are ${role_description}. Based on your knowledge, can you check the correctness of the latest solutions given in the chat history? 

    You should always respond in the following format:
    Thought: your thought
    Reasoning: your reasoning process for checking the correctness
    Criticism: constructive criticism on your and others thought
    Speak: your words to say to others
    Decision: set to "[Agree]" if you think the latest solution is correct, otherwise "[Disagree]"
  # Now give your response.
  # If the solution is correct, end your response with a special token "[Correct]".
  # If the solution is wrong, end your response with a special token "[Wrong]".
  # # Response Format
  # Using the solution from the other member as additional information, can you provide your answer to this math problem? Explain your reasoning. Your final answer should be a single numerical number (not a equation), in the form \boxed{answer}, at the end of your response. And additionally:
  # 1. If your solution has the same answer to the provided solution, end your response with a special token "[Correct]".
  # 2. If your solution has different answer to the provided solution, end your response with a special token "[Wrong]".

  evaluator_prepend_prompt: &evaluator_prepend_prompt |-
    Experts: ${all_role_description}
    Problem: ${task_description}
    Solution: 
    ```
    ${solution}
    ```

  evaluator_append_prompt: &evaluator_append_prompt |-
    You are an experienced mathematic teacher. As a good teacher, you carefully check the correctness of the given solution on a grade school math problem. 
    
    You should respond in the following format:
    Thought: your thought on the problem
    Reasoning: your reasoning process
    Criticism: constructive criticism on your and others' thought
    Speak: your advice to say to others
    Correctness: 0 or 1, 0 is wrong, and 1 is correct

    

name: pipeline


environment:
  env_type: task-basic
  max_turn: *max_turn
  role_assigner:
    type: role_description
    cnt_agents: *cnt_agents
  decision_maker:
    type: vertical-solver-first
  executor:
    type: none
  evaluator:
    type: basic

agents:
  - #role_assigner_agent:
    agent_type: role_assigner
    name: role assigner
    prepend_prompt_template: *role_assigner_prepend_prompt
    append_prompt_template: *role_assigner_append_prompt
    max_retry: 10
    memory:
      memory_type: chat_history
    llm:
      llm_type: gpt-3.5-turbo
      model: "gpt-3.5-turbo"
      temperature: 0
      max_tokens: 512
    output_parser:
      type: role_assigner

  - #solver_agent:
    agent_type: solver
    name: Planner
    prepend_prompt_template: *solver_prepend_prompt
    append_prompt_template: *solver_append_prompt
    max_retry: 10
    memory:
      memory_type: chat_history
    llm:
      llm_type: gpt-3.5-turbo
      model: "gpt-3.5-turbo-16k"
      temperature: 0.3
      max_tokens: 2048
    output_parser:
      type: mgsm-solver-autogpt

  - #critic_agents:
    agent_type: critic
    name: Critic 1
    role_description: |-
      Waiting to be assigned.
    prepend_prompt_template: *critic_prepend_prompt
    append_prompt_template: *critic_append_prompt
    max_retry: 10
    memory:
      memory_type: chat_history
    llm:
      llm_type: gpt-3.5-turbo
      model: "gpt-3.5-turbo-16k"
      temperature: 0
      max_tokens: 2048
    output_parser:
      type: mgsm-critic-autogpt

  - #executor_agent:
    agent_type: executor
    name: Executor
    max_retry: 10
    memory:
      memory_type: chat_history
    llm:
      llm_type: gpt-3.5-turbo
      model: gpt-3.5-turbo
      temperature: 0
      max_tokens: 1024
    output_parser:
      type: mgsm

  - #evaluator_agent:
    agent_type: evaluator
    name: Evaluator
    role_description: |-
      Evaluator
    prepend_prompt_template: *evaluator_prepend_prompt
    append_prompt_template: *evaluator_append_prompt
    max_retry: 10
    memory:
      memory_type: chat_history
    llm:
      llm_type: gpt-3.5-turbo
      model: gpt-3.5-turbo
      temperature: 0.3
      max_tokens: 1024
    output_parser:
      type: mgsm-evaluator-autogpt
      dimensions:
        - Correctness


tools:

