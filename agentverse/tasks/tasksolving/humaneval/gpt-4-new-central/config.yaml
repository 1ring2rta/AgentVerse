cnt_agents: &cnt_agents 4
max_turn: &max_turn 3
max_inner_turns: &max_inner_turns 2

prompts:
  role_assigner_prepend_prompt: &role_assigner_prepend_prompt |-
    # Role Description
    You are the leader of a group of experts, now you need to recruit a small group of experts with diverse identity to correctly write the code to solve the given problems:
    ${task_description}
    
    You can recruit ${cnt_critic_agents} expert in different fields. What experts will you recruit to better generate an accurate solution?

    Here are some suggestion:
    ${advice}
    
  role_assigner_append_prompt: &role_assigner_append_prompt |-
    # Response Format Guidance
    You should respond with a list of ${cnt_critic_agents} expert description. For example:
    1. an electrical engineer specified in the filed of xxx.
    2. an economist who is good at xxx.
    3. a lawyer with a good knowledge of xxx.
    ...

    Only respond with the description of each role. Do not include your reason.
  
  solver_prepend_prompt: &solver_prepend_prompt |-
    ${chat_record}

  solver_append_prompt: &solver_append_prompt |-
    Extract the final code completion they gave in the above chat record. You should return only the python function wrapped with markdown quotes (```python) and nothing else.


  critic_prepend_prompt: &critic_prepend_prompt |-
    Can you complete the following code?
    ```python 
    ${task_description} 
    ```

  critic_append_prompt: &critic_append_prompt |-
    Above is a chat record of an expert team solving the problem. Imagine another team of experts are collaborating to solve it. The members include ${roles}. Generate a chat record that simulates their discussion, working towards a solution.

    The chat record should be formatted as follows:
    [role_1]: ...
    [role_2]: ...
    ...

    The final record should contain the completed Python code solution. Ensure that the code is correctly wrapped in markdown quotes (```python) and contains no additional text.

    Begin your chat simulation:
  

  manager_prompt: &manager_prompt |-
    According to the Previous Solution and the Previous Sentences, select the most appropriate Critic from a specific Role and output the Role.
    ```python 
    ${task_description} 
    ```
    # Previous Solution
    The solution you gave in the last step is:
    ${former_solution}

    # Critics
    There are some critics on the above solution:
    ```
    ${critic_opinions}
    ```

    # Previous Sentences
    The previous sentences in the previous rounds is:
    ${previous_sentence}

  executor_prepend_prompt: &executor_prepend_prompt |-
    You are an experienced program tester. Now your team is trying to solve the problem: 
    '''
    Complete the Python function:
    ${task_description}
    '''

  executor_append_prompt: &executor_append_prompt |-
    The solution has been written to `tmp/main.py`. Your are going to write the unit testing code for the solution. You should respond in the following format:
    Thought: your thought
    Reasoning: your reasoning on the testing cases
    Criticism: constructive self-criticism
    File Path: the path to write your testing code
    Code: the testing code with explaination in docstring. make sure to write the input in the assertion to make it appear in the unit test report, and make sure the expected answer is correct
    Command: the command to change directory and execute your testing code

  evaluator_prepend_prompt: &evaluator_prepend_prompt |-
    # Problem
    Complete the following function
    ```python
    ${task_description}
    ```
    
    # Experts
    The experts recruited in this turn includes:
    ${all_role_description}

    # Writer's Solution: 
    ${solution}

  evaluator_append_prompt: &evaluator_append_prompt |-
    # Response Format Guidance
    You must respond in the following format:
    Score: (0 or 1, 0 for incorrect and 1 for correct)
    Response: (give your advice on how to correct the solution, and your suggestion on what experts should recruit in the next round)
    

name: pipeline


environment:
  env_type: task-basic
  max_turn: *max_turn
  role_assigner:
    type: role_description
    cnt_agents: *cnt_agents
  decision_maker:
    type: central
  executor:
    type: none
  evaluator:
    type: basic

agents:
  - #role_assigner_agent:
    agent_type: role_assigner
    name: role assigner
    max_retry: 1000
    prepend_prompt_template: *role_assigner_prepend_prompt
    append_prompt_template: *role_assigner_append_prompt
    memory:
      memory_type: chat_history
    llm:
      llm_type: gpt-4
      model: "gpt-4"
      temperature: 0
      max_tokens: 512
    output_parser:
      type: role_assigner

  - #solver_agent:
    agent_type: solver
    name: Planner
    max_retry: 1000
    prepend_prompt_template: *solver_prepend_prompt
    append_prompt_template: *solver_append_prompt
    memory:
      memory_type: chat_history
    llm:
      llm_type: gpt-4
      model: "gpt-4"
      temperature: 0
      max_tokens: 2048
    output_parser:
      type: humaneval-solver
      # stop:
      #   - "\ndef "
      #   - "\nclass "
      #   - "\nif "
      #   - "\n\n#"

  - #critic_agents:
    agent_type: critic
    name: Critic 1
    max_retry: 1000
    role_description: |-
      Waiting to be assigned.
    prepend_prompt_template: *critic_prepend_prompt
    append_prompt_template: *critic_append_prompt
    memory:
      memory_type: chat_history
    llm:
      llm_type: gpt-4
      model: "gpt-4"
      temperature: 0
      max_tokens: 2048
    output_parser:
      type: humaneval-critic-central

  - #executor_agent:
    agent_type: executor
    name: Executor
    max_retry: 1000
    prepend_prompt_template: *executor_prepend_prompt
    append_prompt_template: *executor_append_prompt
    memory:
      memory_type: chat_history
    llm:
      llm_type: gpt-4
      model: gpt-4
      temperature: 0
      max_tokens: 1024
    output_parser:
      type: humaneval-executor

  - #evaluator_agent:
    agent_type: evaluator
    name: Evaluator
    max_retry: 1000
    role_description: |-
      Evaluator
    prepend_prompt_template: *evaluator_prepend_prompt
    append_prompt_template: *evaluator_append_prompt
    memory:
      memory_type: chat_history
    llm:
      llm_type: gpt-4
      model: gpt-4
      temperature: 0.3
      max_tokens: 1024
    output_parser:
      type: mgsm-evaluator
      dimensions:
        - Score


  - #manager_agent:
    agent_type: manager
    name: Manager
    max_retry: 1000
    prompt_template: *manager_prompt
    memory:
      memory_type: chat_history
    llm:
      llm_type: gpt-4
      model: "gpt-4"
      temperature: 0
      max_tokens: 1024
    output_parser:
      type: humaneval-manager


