cnt_agents: &cnt_agents 4
max_rounds: &max_rounds 5
max_criticizing_rounds: 3

prompts:
  role_assigner_prepend_prompt: &role_assigner_prepend_prompt |-
    # Role Description
    You are the leader of a group of experts, now you need to recruit a small group of experts with diverse identity to correctly write the code to solve the given problems:
    ${task_description}
    
    You can recruit ${cnt_critic_agents} expert in different fields. What experts will you recruit to better generate an accurate solution?
    
  role_assigner_append_prompt: &role_assigner_append_prompt |-
    # Response Format Guidance
    You should respond with a list of expert description. For example:
    1. an electrical engineer specified in the filed of xxx.
    2. an economist who is good at xxx.
    3. a lawyer with a good knowledge of xxx.
    ...

    Only respond with the description of each role. Do not include your reason.

  solver_prepend_prompt: &solver_prepend_prompt |-
    Can you complete the following code?
    ```python 
    ${task_description} 
    ```

    # Previous Solution
    The solution you gave in the last step is:
    ${former_solution}

    # Critics
    The following messages are the critics on the provided solution:
  
  solver_append_prompt: &solver_append_prompt |-
    Using these information, can you provide a correct completion of the code? Explain your reasoning. Your response should contain only Python code. Do not give any additional information. Use ```python to put the completed Python code in markdown quotes. When responding, please include the given code and the completion.

  critic_prepend_prompt: &critic_prepend_prompt |-
    You are in a discussion group, aiming to complete the following code function:
    ```python
    ${task_description}
    ```

    Below is a possible code completion:
    ```
    ${preliminary_solution}
    ```

  critic_append_prompt: &critic_append_prompt |-
    You are ${role_description}. Based on your knowledge, can you check the correctness of the completion given above? When responding, you should follow the following rules:
    1. If there is a solution, double-check the solution, give your critics. If the provided solution is correct, end your response with a special token "[Agree]", if the solution is incorrect, end with "[Disagree]".
    2. If there is no solution provided yet, please elaborate on what should be noticed when solving this problem, and end your response with a special token "[Disagree]"
  
    Now give your response.

  evaluator_prepend_prompt: &evaluator_prepend_prompt |-
    You are an experienced code reviewer. As a good reviewer, you carefully check the correctness of the given code completion. When the completion is incorrect, you should patiently teach the writer how to correct the completion.
    
    # Problem and Writer's Solution
    Problem: 
    ${task_description}

    Writer's Solution: 
    ${result}

  evaluator_append_prompt: &evaluator_append_prompt |-
    # Response Format Guidance
    You must respond in the following format:
    Score: (0 or 1, 0 for incorrect and 1 for correct)
    Response: (give your advice on how to correct the solution)

    Now carefully check the writer's solution, and give your response.
    

name: pipeline


environment:
  env_type: pipeline
  max_rounds: *max_rounds
  role_assigner:
    type: role_description
    cnt_agents: *cnt_agents
  decision_maker:
    type: vertical
  executor:
    type: none
  evaluator:
    type: basic

agents:
  - #role_assigner_agent:
    agent_type: role_assigner
    name: role assigner
    prepend_prompt_template: *role_assigner_prepend_prompt
    append_prompt_template: *role_assigner_append_prompt
    memory:
      memory_type: chat_history
    llm:
      llm_type: gpt-3.5-turbo
      model: "gpt-3.5-turbo"
      temperature: 0
      max_tokens: 512
    output_parser:
      type: role_assigner

  - #solver_agent:
    agent_type: solver
    name: Planner
    prepend_prompt_template: *solver_prepend_prompt
    append_prompt_template: *solver_append_prompt
    memory:
      memory_type: chat_history
    llm:
      llm_type: gpt-3.5-turbo
      model: "gpt-3.5-turbo"
      temperature: 0
      max_tokens: 1024
    output_parser:
      type: humaneval-solver
      # stop:
      #   - "\ndef "
      #   - "\nclass "
      #   - "\nif "
      #   - "\n\n#"

  - #critic_agents:
    agent_type: critic
    name: Critic 1
    role_description: |-
      Waiting to be assigned.
    prepend_prompt_template: *critic_prepend_prompt
    append_prompt_template: *critic_append_prompt
    memory:
      memory_type: chat_history
    llm:
      llm_type: gpt-3.5-turbo
      model: "gpt-3.5-turbo"
      temperature: 0
      max_tokens: 1024
    output_parser:
      type: mgsm-critic-agree

  - #executor_agent:
    agent_type: executor
    name: Executor
    prompt_template: None
    memory:
      memory_type: chat_history
    llm:
      llm_type: gpt-3.5-turbo
      model: gpt-3.5-turbo
      temperature: 0
      max_tokens: 512
    output_parser:
      type: humaneval

  - #evaluator_agent:
    agent_type: evaluator
    name: Evaluator
    role_description: |-
      Evaluator
    prepend_prompt_template: *evaluator_prepend_prompt
    append_prompt_template: *evaluator_append_prompt
    memory:
      memory_type: chat_history
    llm:
      llm_type: gpt-3.5-turbo
      model: gpt-3.5-turbo
      temperature: 0.3
      max_tokens: 1024
    output_parser:
      type: mgsm-evaluator
      dimensions:
        - Score
