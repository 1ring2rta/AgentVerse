cnt_agents: &cnt_agents 4
cnt_tool_agents: &cnt_tool_agents 3
max_rounds: &max_rounds 5
max_criticizing_rounds: 3

prompts:
  role_assigner_prepend_prompt: &role_assigner_prepend_prompt |-
    # Role Description
    You are the leader of a group of experts, now you need to recruit a small group of experts with diverse identity and apply them with tools to solve the given problems:
    ${task_description}
    
    You can recruit ${cnt_critic_agents} expert in different fields. What experts will you recruit to better generate an accurate solution?

    Here are some suggestion:
    ${advice}
    
  role_assigner_append_prompt: &role_assigner_append_prompt |-
    # Response Format Guidance
    You should respond with a list of expert description. For example:
    1. an electrical engineer specified in the filed of xxx.
    2. an economist who is good at xxx.
    3. a lawyer with a good knowledge of xxx.
    ...

    Only respond with the description of each role. Do not include your reason.

  solver_prepend_prompt: &solver_prepend_prompt |-
    Read the following description of the problem:
    ${task_description} 

    # Previous Solution
    The solution you gave in the last step is:
    ${former_solution}

    # Evaluation
    The unit testing gave the following suggestion:
    ${advice}

    # Critics
    The following messages are the critics on the provided solution:
  
  solver_append_prompt: &solver_append_prompt |-
    Using these information, can you assign several subtasks to the executor?
    You should describe the subtasks in a list. For example:
    1. Look for the weather on the date of 2023-09-10 in New York.
    2. Write a report on the topic of xxx to a file.
    3. Search for the movies that is on theater these days

    Only respond with the description of each subtask. Do not include your reason.

  critic_prepend_prompt: &critic_prepend_prompt |-
    You are in a discussion group, aiming to solve the task:
    ${task_description}
    

    Below is a possible solution:
    ```
    ${preliminary_solution}
    ```

  critic_append_prompt: &critic_append_prompt |-
    You are ${role_description}. Based on your knowledge, can you check the correctness of the completion given above? When responding, you should follow the following rules:
    1. If there is a solution, double-check the solution, give your critics. If the provided solution is correct, end your response with a special token "[Agree]", if the solution is incorrect, end with "[Disagree]".
    2. If there is no solution provided yet, please elaborate on what should be noticed when solving this problem, and end your response with a special token "[Disagree]"
  
    Now give your response.


  manager_prompt: &manager_prompt |-
    According to the Previous Solution and the Previous Sentences, select the most appropriate Critic from a specific Role and output the Role.
    
    ${task_description} 
    
    # Previous Solution
    The solution you gave in the last step is:
    ${former_solution}

    # Critics
    There are some critics on the above solution:
    ```
    ${critic_opinions}
    ```

    # Previous Sentences
    The previous sentences in the previous rounds is:
    ${previous_sentence}

  executor_prepend_prompt: &executor_prepend_prompt |-
    You are in a discussion group aiming to solve the task:
    ${task_description}
    Your team has divided the task into several subtasks:
    '''
    ${solution}
    '''
    Here are the tools given by the team:
    '''
    ${tools}
    '''
  executor_append_prompt: &executor_append_prompt |-
    Please use the tools given by the team to solve the subtasks. You should respond in the following json format wrapped with markdown quotes:
    ```json
    {
      "subtask1": {
        "tool": the name of the tool,
        "args": {
          "arg1(substitute to the name of the argument)": the value of the argument,
          "arg2": the value of the argument,
          ...
        },
      }
      "subtask2": {
        "tool": the name of the tool,
        "args": {
          "arg1": the value of the argument,
          "arg2": the value of the argument,
          ...
        },
      }
      "subtask3": {
        "tool": the name of the tool,
        "args": {
          "arg1": the value of the argument,
          "arg2": the value of the argument,
          ...
        },
      }
    }
    ```
    Respond only the json, and nothing else. You can use the same tool for different subtasks.
  evaluator_prepend_prompt: &evaluator_prepend_prompt |-
    You are an experienced code reviewer. As a good reviewer, you carefully check the correctness of the given code completion. When the completion is incorrect, you should patiently teach the writer how to correct the completion.

    # Experts
    The experts recruited in this turn includes:
    ${all_role_description}
    
    # Problem and Writer's Solution
    Problem: 
    ${task_description}

    Writer's Solution: 
    ${solution}

    Tester's Feedback:
    ${result}

  evaluator_append_prompt: &evaluator_append_prompt |-
    # Response Format Guidance
    You must respond in the following format:
    Score: (0 or 1, 0 for incorrect and 1 for correct)
    Response: (give your advice on how to correct the solution, and your suggestion on on what experts should recruit in the next round)

    Now carefully check the writer's solution, summarize the tester's feedback, and give your response.
    

name: pipeline


environment:
  env_type: pipeline
  max_rounds: *max_rounds
  role_assigner:
    type: role_description
    cnt_agents: *cnt_agents
  decision_maker:
    type: horizontal
  executor:
    type: tool-using
    num_agents: *cnt_tool_agents
    tools: #TODO
       tool1: 
         name: web_requests
         decription: make http requests
         args:
            url: https://www.google.com
            content: 
       tool2: 
         name: analyze_code
         decription: analyze code
         args :
            file_path: ./test.py
            text : 
       tool3: 
         name: execute_code
         decription: execute code
         args:
           file_path: ./test.py
       tool4: 
         name: write_tests
         decription: write tests
         args:
           file_path: ./test.py
       tool5: 
         name: improve_code
         decription: improve code
         args:
           file_path: ./test.py
       tool6: 
         name: file_operations
         decription: operate files
         args:
           file_path: ./test.py
       tool7: 
         name: git_operations
         decription: operate git
         args:
           file_path: ./test.py
       tool8: 
         name: web_playwright
         decription: operate playwright
         args:
           url: https://www.google.com
           content:
  evaluator:
    type: basic

agents:
  - #role_assigner_agent:
    agent_type: role_assigner
    name: role assigner
    prepend_prompt_template: *role_assigner_prepend_prompt
    append_prompt_template: *role_assigner_append_prompt
    memory:
      memory_type: chat_history
    llm:
      llm_type: gpt-3.5-turbo
      model: "gpt-3.5-turbo"
      temperature: 0
      max_tokens: 512
    output_parser:
      type: role_assigner

  - #solver_agent:
    agent_type: solver
    name: Planner
    prepend_prompt_template: *solver_prepend_prompt
    append_prompt_template: *solver_append_prompt
    memory:
      memory_type: chat_history
    llm:
      llm_type: gpt-3.5-turbo
      model: "gpt-3.5-turbo-16k"
      temperature: 0
      max_tokens: 1024
    output_parser:
      type: toolusing-solver
      # stop:
      #   - "\ndef "
      #   - "\nclass "
      #   - "\nif "
      #   - "\n\n#"

  - #critic_agents:
    agent_type: critic
    name: Critic 1
    role_description: |-
      Waiting to be assigned.
    prepend_prompt_template: *critic_prepend_prompt
    append_prompt_template: *critic_append_prompt
    memory:
      memory_type: chat_history
    llm:
      llm_type: gpt-3.5-turbo
      model: "gpt-3.5-turbo"
      temperature: 0
      max_tokens: 1024
    output_parser:
      type: mgsm-critic-agree

  - #executor_agent:
    agent_type: executor
    name: Executor
    prepend_prompt_template: *executor_prepend_prompt
    append_prompt_template: *executor_append_prompt
    memory:
      memory_type: chat_history
    llm:
      llm_type: gpt-3.5-turbo
      model: gpt-3.5-turbo
      temperature: 0
      max_tokens: 1024
    output_parser:
      type: humaneval-executor

  - #evaluator_agent:
    agent_type: evaluator
    name: Evaluator
    role_description: |-
      Evaluator
    prepend_prompt_template: *evaluator_prepend_prompt
    append_prompt_template: *evaluator_append_prompt
    memory:
      memory_type: chat_history
    llm:
      llm_type: gpt-3.5-turbo
      model: gpt-3.5-turbo
      temperature: 0.3
      max_tokens: 1024
    output_parser:
      type: mgsm-evaluator
      dimensions:
        - Score


  - #manager_agent:
    agent_type: manager
    name: Manager
    prompt_template: *manager_prompt
    memory:
      memory_type: chat_history
    llm:
      llm_type: gpt-3.5-turbo
      model: "gpt-3.5-turbo"
      temperature: 0
      max_tokens: 1024
    output_parser:
      type: humaneval-manager


