<h1 align="center"> ü§ñ AgentVerse ü™ê </h1>

<h3 align="center">
    <p>A Framework for Multi-LLM Environment Simulation</p>
</h3>
<p align="center">
    <a href="https://github.com/OpenBMB/AgentVerse/blob/main/LICENSE">
        <img alt="License: Apache2" src="https://img.shields.io/badge/License-Apache_2.0-green.svg">
    </a>
    <a href="https://www.python.org/downloads/release/python-3916/">
        <img alt="Documentation" src="https://img.shields.io/badge/python-3.9+-blue.svg">
    </a>
</p>

<p align="center">
<img src="./imgs/title.png" width="512">
</p>

**AgentVerse** offers a versatile framework that streamlines the process of creating custom multi-agent environments for large language models (LLMs). Designed to facilitate swift development and customization with minimal effort, our framework empowers researchers to concentrate on their research, rather than being bogged down by implementation details.

---

## 1. Quick Start
```bash
export OPENAI_API_KEY=sk-xxxxxxxxxxxxxx

python benchmark.py --task humaneval/gpt-3.5-new --dataset_path data/humaneval/test.jsonl --overwrite
```
This will launch the pipeline on the `humaneval` dataset, which is a benchmark for evaluating the performance of LLMs on code completion.

## 2. Overall Architecture
The overall logic is in the `step` function from `agentverse/environment/pipeline.py`. Where the four stages: `role_assign`, `decision_making`, `execution` and `evaluation`, are called sequentially. 

We have separated the logic of each stage into different directories, which are located in the `agentverse/environments` directory (the `rules` directory is useless in this repo). 

### 2.1 Role Assignment
#### Input
- task_description: the description of the task
- advice: the advice from the evaluator in the last round

#### Output: 
- a list of agents, each agent has a different `role_description` filled into their prompt.

Basically, the role assignment accept the input including `task_description` and the `advice` from the evaluator in the last round, and the role assigner (powered by a LLM) generates the `role description` for each agent. These `role_description` are filled into the prompt of each agent. The agents are then sent to the next stage.

### 2.2 Decision Making
#### Input:
- task_description: the description of the task
- previous_plan: the decision made in the last round
- advice: the advice from the evaluator in the last round
#### Output:
- a list of actions, each action is a string.

The agents will discuss on the provided task and then generate a list of actions. For example, in humaneval dataset, we use the `vertical_solver_first` decision making structure, where one agent acts as "solver", and others act as "reviewer". The solver first gives a code completion, and the reviewers provide feedback, and the solver modify the completion. The iteration repeats until all the reviewers reach consensus or the maximum iteration count is reached.

### 2.3 Action Execution
#### Input:
- task_description: the description of the task
- solutions: the list of actions generated by the agents in the last stage. we did not name it "action", it is just an issue left over from the previous version.
#### Output:
- a list of action result.

Currently in our code, we assume that only one action is given from the decision making stage (because we are doing some tasks such as code completion or math problem solving, which do not require multiple solution(action)).

### 2.4 Evaluation
#### Input:
- solution: the action generated by the agents.
- result: the result of the action execution stage.
- task_description: the description of the task
- all_role_description: the role description of all the agents in this round.
#### Output:
- scores: the scores of different aspects that is configured in the config file
- advice: the advice for the agents in the next round.

## 3. Misc
To se how the agents and environments are configured, you can refer to `agentverse/tasks/humaneval/gpt-3.5-new/config.yaml`. Each agent has two prompts, a prepend prompt and an append prompt. The final message that is passed to the LLM would be of the structure: 
```python
[
  {
    "role": "System",
    "content": prepend_prompt
  },
  *[
    # a list of their conversation messages
  ],
  {
    "role": "user",
    "content": append_prompt
  }
]
```

<!-- ## Contents

- [‚ú® Features](#-features)
- [üì∞ What's New](#-whats-new)
- [üóì Coming Soon](#-coming-soon)
- [üëæ Simple Demo Video](#-simple-demo-video)
    - [NLP Classroom](#nlp-classroom)
    - [Prisoner Dilemma](#prisoner-dilemma)
    - [Software Design](#software-design)
    - [Database Administrator (DBA)](#database-administrator-dba)
    - [Pokemon](#pokemon)
- [Contents](#contents)
- [üöÄ Getting Started](#-getting-started)
  - [Installation](#installation)
  - [CLI Example](#cli-example)
  - [Local Website Demo](#local-website-demo)
- [üí° Philosophy](#-philosophy)
  - [Environment](#environment)
  - [Agent](#agent)
- [‚úçÔ∏è Customize Your Own Environment](#Ô∏è-customize-your-own-environment)
  - [A Simple Example: Building a Classroom Environment](#a-simple-example-building-a-classroom-environment)
      - [1. Creating a Task Directory and Configuring the Environment](#1-creating-a-task-directory-and-configuring-the-environment)
      - [2. Configuring the Agents](#2-configuring-the-agents)
      - [3. Writing an Output Parser](#3-writing-an-output-parser)
  - [Customization Guide for More Complex Environments](#customization-guide-for-more-complex-environments)
- [üîé Examples](#-examples) -->



<!-- ## üöÄ Getting Started

### Installation

```bash
pip install -U agentverse
```
Or you can install the package by manually cloning the latest repository
```bash
git clone https://github.com/OpenBMB/AgentVerse.git --depth 1
cd AgentVerse
pip install -r requirements.txt
```
Some users have reported problems installing the `orjson` required by `gradio`. One simple workaround is to install it with Anaconda `conda install -c conda-forge orjson`.

You also need to export your OpenAI API key as follows
```bash
# Export your OpenAI API key
export OPENAI_API_KEY="your_api_key_here"
```


### AgentVerse-ProblemSolving Example


### Consulting

#### Multi-Agent

python3 pipeline.py --task pipeline_projectv --discussion_mode

```bash
export OPENAI_API_KEY=YOUR_API_KEY
export LOG_LEVEL=INFO  choose one of them {DEBUG, INFO, WARNING, ERROR, CRITICAL}
python3 pipeline.py --task pipeline_brainstorming --discussion_mode
```

#### Single-Agent

python3 pipeline.py --task pipeline_projectv --single_agent

```bash
export OPENAI_API_KEY=YOUR_API_KEY
export LOG_LEVEL=INFO  choose one of them {DEBUG, INFO, WARNING, ERROR, CRITICAL}
python3 pipeline.py --task pipeline_brainstorming --single_agent
```

With the `--discussion_mode` (or `-d` in short) parameter, the pipeline will enter the p-p discussion mode,
otherwise enter criticizing mode (which is used in the `pipeline_pythoncalculator` task).

With the `--single_agent` (or `-s` in short) parameter, the pipeline will enter single agent CoT mode.


### Software Development

#### multi-Agent 

python3 pipeline.py --task pipeline_projectv

```bash
export OPENAI_API_KEY=your api key
export LOG_LEVEL=INFO  choose one of them {DEBUG, INFO, WARNING, ERROR, CRITICAL}
python3 pipeline.py --task pipeline_pythoncalculator
```

#### Single-Agent 

```bash
export OPENAI_API_KEY=your api key
export LOG_LEVEL=INFO  choose one of them {DEBUG, INFO, WARNING, ERROR, CRITICAL}
python3 pipeline.py --task pipeline_pythoncalculator --single_agent
```


**Notice**: LOG_LEVEL = DEBUG will output all prompts to the terminal and log file.
LOG_LEVEL = INFO will only output the parsed response and the pipeline stage info.



### Contact
- Weize Chen: chenwz21@mails.tsinghua.edu.cn
- Yusheng Su: yushengsu.thu@gmail.com -->
